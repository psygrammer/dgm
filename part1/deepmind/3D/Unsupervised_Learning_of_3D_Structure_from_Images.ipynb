{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning of 3D Structure from Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 싸이그래머 / DGM : 파트 1 - DeepMind 논문리뷰 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Contents\n",
    "* Abstract\n",
    "* 1 Introduction\n",
    "* 2 Conditional Generative Models\n",
    "    - 2.1 Architectures\n",
    "* 3 Experiments\n",
    "    - 3.1 Generating volumes\n",
    "    - 3.2 Probabilistic volume completion and denoising\n",
    "    - 3.3 Conditional volume generation\n",
    "    - 3.4 Performance benchmarking\n",
    "    - 3.5 Multi-view training\n",
    "    - 3.6 Single-view training\n",
    "* 4 Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [3] (slide) Unsupervised Learning of 3D Structure from Images -  https://www.cs.toronto.edu/~duvenaud/courses/csc2541/slides/unsupervised-3d.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Conditional Generative Models\n",
    "* 2.1 Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denote the set of all parameters of this generative model as θ = {θr , θw , θs , θp }. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap4.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3D → 3D projection (identity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see figure 3(left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3D → 2D neural projection (learned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see figure 3(middle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3D → 2D OpenGL projection (fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see figure 3(right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Experiments\n",
    "* 3.1 Generating volumes\n",
    "* 3.2 Probabilistic volume completion and denoising\n",
    "* 3.3 Conditional volume generation\n",
    "* 3.4 Performance benchmarking\n",
    "* 3.5 Multi-view training\n",
    "* 3.6 Single-view training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [2] (vidoe) Unsupervised Learning of 3D Structure from Images - https://www.youtube.com/watch?v=stvDAGQwL5c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrate the ability of our model to learn and exploit 3D scene representations in five challenging tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Necker cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 참고\n",
    "* [4] (vidoe) The Necker Cube - https://www.youtube.com/watch?v=fEN8YAXdOak\n",
    "* [5] (wikipedia) The Necker Cube - https://en.wikipedia.org/wiki/Necker_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the simplest dataset we use and consists of 40 × 40 × 40 volumes with a 10 × 10 × 10 wire-frame cube drawn at a random orientation at the center of the volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primitives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The volumetric primitives are of size 30 × 30 × 30. \n",
    "* Each volume contains a simple solid geometric primitive (e.g., cube, sphere, pyramid, cylinder, capsule or ellipsoid) that undergoes random translations ([0, 20] pixels) and rotations ([−π, π] radians)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We extended the MNIST dataset [22] to create a 30 × 30 × 30 volumetric dataset by extruding the MNIST images.\n",
    "* The resulting dataset has the same number of images as MNIST. \n",
    "* The data is then augmented with random translations ([0, 20] pixels) and rotations ([−π, π] radians) that are procedurally applied during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ShapeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 참고\n",
    "* [6] ShapeNet - http://shapenet.cs.stanford.edu/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ShapeNet dataset is a large dataset of 3D meshes of objects. \n",
    "* We experiment with a 40-class subset of the dataset, commonly referred to as ShapeNet40. \n",
    "* We render each mesh as a binary 30 × 30 × 30 volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all experiments we used LSTMs with 300 hidden neurons and 10 latent variables per generation step. \n",
    "* The context encoder $f_c$(c,$s_{t−1}$) was varied for each task.\n",
    "* For image inputs \n",
    "    - we used convolutions and standard spatial transformers, and \n",
    "* for volumes \n",
    "    - we used volumetric convolutions and VSTs. \n",
    "* For the class-conditional experiments, \n",
    "    - the context c is a one-hot encoding of the class. \n",
    "* As meshes are much lower-dimensional than volumes, \n",
    "    - we set the number of steps to be T = 1 \n",
    "        - when working with this representation. \n",
    "* We used the Adam optimizer for all experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Generating volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When ground-truth volumes are available we can directly train the model using the identity projection operator (see section 2.1). \n",
    "* We explore the performance of our model by training on several datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap5.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Probabilistic volume completion and denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We test the ability of the model to impute missing data in 3D volumes.\n",
    "* This procedure simulates a Markov chain and samples from the correct distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Conditional volume generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models can also be trained with context representing the class of the object, allowing for class conditional generation. We train a class-conditional model on ShapeNet and show multiple samples for 10 of the 40 classes in figure 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also form conditional models using a single view of 2D contexts. Our results, shown in figure 8 indicate that the model generates plausible shapes that match the constraints provided by the context and captures the multi-modality of the posterior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Performance benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We quantify the performance of the model by computing likelihood scores, varying the number of conditioning views and the number of inference steps in the model. \n",
    "* Figure 6 indicates that the number of generation steps is a very important factor for performance. \n",
    "* Additional context views generally improve the model’s performance but the effect is relatively small. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Multi-view training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In most practical applications, ground-truth volumes are not available for training. \n",
    "* Instead, data is captured as a collection of images (e.g., from a multi-camera rig or a moving robot). \n",
    "* To accommodate this fact, we extend the generative model with a <font color=\"red\">projection operator</font> that <font color=\"blue\">maps the internal volumetric representation $h_T$ to a 2D image $\\hat{x}$</font>. \n",
    "    - This map imitates a ‘camera’ in that it first applies an affine transformation to the volumetric representation, and then flattens the result using a convolutional network.\n",
    "    - The parameters of this projection operator are trained jointly with the rest of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from fixed camera locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this experiment <font color=\"red\">we train the model to learn to reproduce an image of the object</font> <font color=\"blue\">given one or more views of it from fixed camera locations</font>. \n",
    "* It is the model’s responsibility to infer the volumetric representation as well as the camera’s position relative to the volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a model that conditions on 3 fixed context views to reproduce 10 simultaneous random views of an object.\n",
    "* After training, we can sample a 3D representation given the context, and render it from arbitrary camera angles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Single-view training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we consider a mesh-based 3D representation and demonstrate the feasibility of training our models with a fully-fledged, black-box renderer in the loop. \n",
    "* Such renderers (e.g. OpenGL) accurately capture the relationship between a 3D representation and its 2D rendering out of the box. \n",
    "* This image is a complex function of the objects’ colors, materials and textures, positions of lights, and that of other objects. \n",
    "* By building this knowledge into the model we give hints for learning and constrain its hidden representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">We consider again the Primitives dataset, however now we only have access to 2D images of the objects at training time</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap11.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "* [1] (Paper) Unsupervised Learning of 3D Structure from Images - https://arxiv.org/abs/1607.00662\n",
    "* [2] (vidoe) Unsupervised Learning of 3D Structure from Images - https://www.youtube.com/watch?v=stvDAGQwL5c\n",
    "* [3] (slide) Unsupervised Learning of 3D Structure from Images -  https://www.cs.toronto.edu/~duvenaud/courses/csc2541/slides/unsupervised-3d.pdf\n",
    "* [4] (vidoe) The Necker Cube - https://www.youtube.com/watch?v=fEN8YAXdOak\n",
    "* [5] (wikipedia) The Necker Cube - https://en.wikipedia.org/wiki/Necker_cube\n",
    "* [6] ShapeNet - http://shapenet.cs.stanford.edu/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
