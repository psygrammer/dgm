{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Reinforcement Learnning Problem\n",
    "- DGM: 파트 1 - 강화학습 기초[2]\n",
    "- 발표자: 정권우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고\n",
    "- [1] Reinforcement Learning: An Introduction (second edition) - http://incompleteideas.net/sutton/book/the-book.html\n",
    "- [2] (slides) CMPUT 609: Reinforcement Learning for Artificial Intelligence - http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/RLAIcourse/2010.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "### Chapter 2 _ Multi-arm Bandits\n",
    "- 2.1 A $k$-armed Bandit Problem\n",
    "- 2.2 Action-Value Methods\n",
    "- 2.3 Incremental Implementation\n",
    "- 2.4 Tracking a Nonstationary Problem\n",
    "- 2.5 Optimistic Initial Values\n",
    "- 2.6 Upper-Confidence-Bound Action Selection\n",
    "- 2.7 Gradient Bandit Algorithms\n",
    "- 2.8 Associative Search (Contextual Bandits)\n",
    "- 2.9 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Intro\n",
    "#### Evaluative vs Instructive\n",
    "- The most important feature distinguishing reinforcement learning from other types of learing is that it uses training information that *evaluates* the actions taken rather than *instructs* by giving correct actions. \n",
    "  - Purely evaluative feedback indicates how good the action taken is, but not whether it is the best or the worst action possible.\n",
    "  - Purely instructive feedbak, on the other hand, indicates the correct action to take, independently of the action actually taken.\n",
    "\n",
    "- In this chapter, we study the *evaluative aspect of reinforcement learning* in a simplified setting, one that does not involve learning to act in more than one situation.\n",
    "\n",
    "- The nonassociative, evaluative feedback problem that we explore is a simple version of the *$k$-armed bandit problem*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 A $k$-Armed Bandit Problem\n",
    "<div align=\"center\">\n",
    "  <img src=\"img/k-armed bandit.png\">\n",
    "</div>\n",
    "- Repeat a choice among $k$ different options or actions.  After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections or *time steps*.\n",
    "- This is the original form of *$k$-armed bandit problem*, so named by analogy to a slot machine. Today, the term \"bandit problem\" is used for a generalization of the problem described above.\n",
    "- In our $k$-armed bandit problem, each of the $k$ actions has an expected or mean reward given that that action is selected; let us call this the *value* of that action.\n",
    "\n",
    "#### Action Value\n",
    "$q_*(a) = E[R_t|A_t=a]$\n",
    "- $q_*(a)$ value of an arbitrary action\n",
    "- $A_t$ action selected on time step t\n",
    "- $R_t$ corresponding reward\n",
    "- Assume we do not know the action values with certainty, but only estimates.\n",
    "- Action Value of action $a$ at time $t$ is $Q_t(a)≈q_*(a)$\n",
    "\n",
    "#### Exploiting vs Exploring\n",
    "<div align=\"center\">\n",
    "  <img src=\"img/exploit-explore.png\">\n",
    "</div>\n",
    "- Greedy actions: choosing an action with highest estimated action value\n",
    "- Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.\n",
    "- If you have many time steps ahead on which to make actions, better to explore nongreedy actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Action-Value Methods\n",
    "\n",
    "- Simple method for estimating the values of actions and for using the estimates to make action selection decisions.\n",
    "- Recall that true value of an action is the mean reward when that action is selected.\n",
    "\n",
    "#### Sample-average method\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"img/eq2.1.png\">\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "- As the denominator goes to infinity, by the law of large numbers, $Q_t(a)$ converges to $q_*(a)$\n",
    "\n",
    "#### Action Selection Rule\n",
    "- The simplest action selection rule is to select the action with highest estimated action value (greedy)\n",
    "\n",
    "$A_t≐argmax_aQ_t(a)$\n",
    "\n",
    "- *ε-greedy* method: Behave greedily most of the time, but with probability ε, select randomly from all actions with equal probability.\n",
    "\n",
    "#### Reward distribution of 10-armed bandit problem\n",
    "<div align=\"center\">\n",
    "  <img src=\"img/fig2.1.png\">\n",
    "</div>\n",
    "\n",
    "#### greedy vs ε-greedy\n",
    "<div align=\"center\">\n",
    "  <img src=\"img/fig2.2.png\">\n",
    "</div>\n",
    "\n",
    "- greedy method performs significantly worse in the long run because it is stuck in suboptimal actions.\n",
    "- ε=0.01 eventually performs better than ε=0.1\n",
    "- Dynamic ε ~ learning rate policy in deep learning\n",
    "\n",
    "#### ε-greedy > greedy is not always True\n",
    "- if reward variance is 10 instead of 1(noisier reward), it takes more exploration to find the optimal action > ε-greedy\n",
    "- if reward variance is 0, no exploration needed > greedy\n",
    "\n",
    "#### Nonstationarity\n",
    "- The true values of the actons change over time.\n",
    "- Most commonly encountered RL problem is nonstationary.\n",
    "- Requires a balance between exploration and exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Incremental Implementation\n",
    "\n",
    "#### Computation of Action Values $Q_n$\n",
    "<div align=\"center\">\n",
    "  <img src=\"img/eq2.3.png\">\n",
    "</div>\n",
    "\n",
    "#### Update rule\n",
    "*NewEstimate <- Old Estimate + StepSize[Target - OldEstimate]*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Tracking a Nonstationary Problem\n",
    "\n",
    "#### Nonstationarity\n",
    "- The average methods are appropriate in stationary environment, but not if the bandit is changing over time.\n",
    "- In such cases, it makes sense to weight recent rewards more heavily than long-past ones.\n",
    "- Most common practice is to use a *constant step-size parameter*. (*exponential, recency-weighted average*)\n",
    "<div align=\"center\">\n",
    "  <img src=\"img/eq2.5.png\">\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "  <img src=\"img/eq2.6.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Optimistic Initial Values\n",
    "#### Initial values encouraging exploration\n",
    "- Usually, we set initial action values to Gaussian distribution with 0 mean, unit variance.\n",
    "- Suppose we set them all to +5\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"img/fig2.3.png\">\n",
    "</div>\n",
    "\n",
    "- Quite effective on stationary problem, but it is far from being a generally useful approach to encouraging exploration. Due to its effect being inherently temporary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Upper-Confidence-Bound Action Selection\n",
    "\n",
    "- Exploration is needed because the estimates of the action values are uncertain.\n",
    "- Select among non-greedy actions according to their potential for actually being optimal, rather than random.\n",
    "\n",
    "#### UCB action selection\n",
    "<div align=\"center\">\n",
    "  <img src=\"img/eq2.8.png\">\n",
    "</div>\n",
    "\n",
    "- Square-root term is a measure of the uncertainty or variance in the estimate of $a$'s value\n",
    "- Each time $a$ is selected, the uncertainty is presumabley reduce; $N_t(a)$ is incremented, uncertainty is decreased.\n",
    "- Each time an action other than $a$ is selected $t$ is increased, and uncertainty estimate is increased.\n",
    "\n",
    "#### UCB vs ε-greedy\n",
    "<div align=\"center\">\n",
    "  <img src=\"img/fig2.4.png\">\n",
    "</div>\n",
    "\n",
    "- In more advanced settings, there is currently no known practical way of utilizing the idea of UBC action selection.\n",
    "  - Nonstationarity requires more complex method of update rule\n",
    "  - Too large state spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Gradient Bandit Algorithms\n",
    "\n",
    "#### Preference as action selection\n",
    "- Learning a numerical *preference* $H_t(a)$ for each action $a$.\n",
    "- The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of rewards.(relative preference)\n",
    "<div align=\"center\">\n",
    "  <img src=\"img/eq2.9.png\">\n",
    "</div>\n",
    "\n",
    "#### Stochastic gradient ascent as learning algorithm\n",
    "<div align=\"center\">\n",
    "  <img src=\"img/eq2.10.png\">\n",
    "</div>\n",
    "\n",
    "#### Performance of gradient bandit algorithm\n",
    "<div align=\"center\">\n",
    "  <img src=\"img/fig2.5.png\" >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Associative Search (Contextual Bandits)\n",
    "\n",
    "#### Nonassociative vs Associative\n",
    "- In *Nonassociative* task, there is no need to associate different actions with different situations\n",
    "- In *Associative* task, there is more than one situation and the goal is to learn a policy: a mapping from situations to the actons that are best in those situations.\n",
    "- Example, 10 random $k$-armed bandit tasks, where bandit changes randomly from play to play. Assume the color of bandit is given as an information, then you ca learn a policy associating each task signaled by the color of bandit.\n",
    "\n",
    "#### Associative Search\n",
    "- trial-and-error learning in the form of *search* for the best actions\n",
    "- *association* of these actions with the situations in which they are best\n",
    "\n",
    "#### Full Reinforcement Learning Problem\n",
    "- Involves learning a *Policy*, due to multiple situations. (Actions are allowed to affect *next situation* as well as the reward)\n",
    "- Usually nonstationary (True value of an action changes over time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Summary\n",
    "\n",
    "#### Ways of balancing Exploration & Exploitation\n",
    "- Greedy method : always choose highest value\n",
    "- ε-greedy method : choose non-greedy randomly with probability ε\n",
    "- UBC : subtly favors actions that have fewer samples\n",
    "- Gradient bandit algorithm : use action preference as action selection method\n",
    "- Optimistic initial values encourage even greedy methods to explore\n",
    "\n",
    "#### Best method to use\n",
    "- Difficult to answer\n",
    "  - Case-by-case (Action-State space size, Nature of problem)\n",
    "  - Each have different hyper parameter to be tuned\n",
    "  \n",
    "<div align=\"center\">\n",
    "  <img src=\"img/fig2.6.png\">\n",
    "</div>\n",
    "\n",
    "#### State of the art\n",
    "- Methods presented in this chapter can be considered the state of the art.\n",
    "- There exists sophisticated methods, but their complexity and assumptions make them impractical for the full reinforcement learning problem that is our real focus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.engadget.com/2016/09/22/facebook-and-intel-reign-supreme-in-doom-ai-deathmatch/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
